Lecture 3.1 - Node Embeddings

https://www.youtube.com/watch?v=rMq21iY61SE&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=7

Lecture 3.2 - Random Walk Approaches for Node Embeddings

https://www.youtube.com/watch?v=Xv0wRy66Big&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=8

Today's lecture talks about random walk approaches for node embeddings. Starting from the objective function, which is built on the assumptions that neighbourhood nodes should have embeddings close together (measured by cosine distance, that is, dot multiplication of two vectors). The objective function is defined by maximizing the likelihood of predicting the neighbourhood nodes given the embeddings of the node. Softmax calculates the probability, Jure briefly covers how to improve the algorithm's efficiency through negative sampling. The optimizing method is simply stochastic gradient descent.
Furthermore, Jure talks about the advances for the naive Random Walk approach, node2vec. Compared to the Random Walk, node2vec introduces 2nd order information: which node it hops from during random walk and which direction it is more likely to hop to. The tendency hops in a BFS or a DFS way, and the possibility it hops back to the original node is defined by two hyperparameters, q and p.

Lecture 3.3 - Embedding Entire Graphs

https://www.youtube.com/watch?v=eliMLfJeu7A&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn&index=9

Jure Leskovec talked about methods for graph-level embeddings, which were conducted through the Anonymous Random Walk approach. Other parts of the algorithm are analogical to DeepWalk and Node2Vec.

