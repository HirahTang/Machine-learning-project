{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [13:26:34] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:27:58] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "tox21_tasks, tox21_datasets, transformers = dc.molnet.load_tox21()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tox21_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tox21_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = tox21_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6264, 1024), (783, 1024), (784, 1024))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.X.shape, valid_dataset.X.shape, test_dataset.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6264, 12), (783, 12), (784, 12))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.y.shape, valid_dataset.y.shape, test_dataset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6264, 12)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04502242, 1.03632599, 1.12502653, 1.04541485, 1.14607755,\n",
       "       1.05621135, 1.02555168, 1.17267917, 1.03538545, 1.05576503,\n",
       "       1.17464996, 1.05288369])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04502242, 1.03632599, 1.12502653, 1.04541485, 1.14607755,\n",
       "       1.05621135, 1.02555168, 1.17267917, 1.03538545, 1.05576503,\n",
       "       1.17464996, 1.05288369])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.w[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NR-AR',\n",
       " 'NR-AR-LBD',\n",
       " 'NR-AhR',\n",
       " 'NR-Aromatase',\n",
       " 'NR-ER',\n",
       " 'NR-ER-LBD',\n",
       " 'NR-PPAR-gamma',\n",
       " 'SR-ARE',\n",
       " 'SR-ATAD5',\n",
       " 'SR-HSE',\n",
       " 'SR-MMP',\n",
       " 'SR-p53']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tox21_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CC(O)(P(=O)(O)O)P(=O)(O)O'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<deepchem.trans.transformers.BalancingTransformer at 0x7fc413a3ff70>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MultitaskClassifier in module deepchem.models.fcnet:\n",
      "\n",
      "class MultitaskClassifier(deepchem.models.keras_model.KerasModel)\n",
      " |  MultitaskClassifier(n_tasks: int, n_features: int, layer_sizes: Sequence[int] = [1000], weight_init_stddevs: Union[float, Sequence[float]] = 0.02, bias_init_consts: Union[float, Sequence[float]] = 1.0, weight_decay_penalty: float = 0.0, weight_decay_penalty_type: str = 'l2', dropouts: Union[float, Sequence[float]] = 0.5, activation_fns: Union[Callable, str, Sequence[Union[Callable, str]]] = <function relu at 0x7fc42a5c9280>, n_classes: int = 2, residual: bool = False, **kwargs) -> None\n",
      " |  \n",
      " |  A fully connected network for multitask classification.\n",
      " |  \n",
      " |  This class provides lots of options for customizing aspects of the model: the\n",
      " |  number and widths of layers, the activation functions, regularization methods,\n",
      " |  etc.\n",
      " |  \n",
      " |  It optionally can compose the model from pre-activation residual blocks, as\n",
      " |  described in https://arxiv.org/abs/1603.05027, rather than a simple stack of\n",
      " |  dense layers.  This often leads to easier training, especially when using a\n",
      " |  large number of layers.  Note that residual blocks can only be used when\n",
      " |  successive layers have the same width.  Wherever the layer width changes, a\n",
      " |  simple dense layer will be used even if residual=True.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MultitaskClassifier\n",
      " |      deepchem.models.keras_model.KerasModel\n",
      " |      deepchem.models.models.Model\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_tasks: int, n_features: int, layer_sizes: Sequence[int] = [1000], weight_init_stddevs: Union[float, Sequence[float]] = 0.02, bias_init_consts: Union[float, Sequence[float]] = 1.0, weight_decay_penalty: float = 0.0, weight_decay_penalty_type: str = 'l2', dropouts: Union[float, Sequence[float]] = 0.5, activation_fns: Union[Callable, str, Sequence[Union[Callable, str]]] = <function relu at 0x7fc42a5c9280>, n_classes: int = 2, residual: bool = False, **kwargs) -> None\n",
      " |      Create a MultitaskClassifier.\n",
      " |      \n",
      " |      In addition to the following arguments, this class also accepts\n",
      " |      all the keyword arguments from TensorGraph.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n_tasks: int\n",
      " |        number of tasks\n",
      " |      n_features: int\n",
      " |        number of features\n",
      " |      layer_sizes: list\n",
      " |        the size of each dense layer in the network.  The length of\n",
      " |        this list determines the number of layers.\n",
      " |      weight_init_stddevs: list or float\n",
      " |        the standard deviation of the distribution to use for weight\n",
      " |        initialization of each layer.  The length of this list should\n",
      " |        equal len(layer_sizes).  Alternatively this may be a single\n",
      " |        value instead of a list, in which case the same value is used\n",
      " |        for every layer.\n",
      " |      bias_init_consts: list or float\n",
      " |        the value to initialize the biases in each layer to.  The\n",
      " |        length of this list should equal len(layer_sizes).\n",
      " |        Alternatively this may be a single value instead of a list, in\n",
      " |        which case the same value is used for every layer.\n",
      " |      weight_decay_penalty: float\n",
      " |        the magnitude of the weight decay penalty to use\n",
      " |      weight_decay_penalty_type: str\n",
      " |        the type of penalty to use for weight decay, either 'l1' or 'l2'\n",
      " |      dropouts: list or float\n",
      " |        the dropout probablity to use for each layer.  The length of this list should equal len(layer_sizes).\n",
      " |        Alternatively this may be a single value instead of a list, in which case the same value is used for every layer.\n",
      " |      activation_fns: list or object\n",
      " |        the Tensorflow activation function to apply to each layer.  The length of this list should equal\n",
      " |        len(layer_sizes).  Alternatively this may be a single value instead of a list, in which case the\n",
      " |        same value is used for every layer.\n",
      " |      n_classes: int\n",
      " |        the number of classes\n",
      " |      residual: bool\n",
      " |        if True, the model will be composed of pre-activation residual blocks instead\n",
      " |        of a simple stack of dense layers.\n",
      " |  \n",
      " |  default_generator(self, dataset: deepchem.data.datasets.Dataset, epochs: int = 1, mode: str = 'fit', deterministic: bool = True, pad_batches: bool = True) -> Iterable[Tuple[List, List, List]]\n",
      " |      Create a generator that iterates batches for a dataset.\n",
      " |      \n",
      " |      Subclasses may override this method to customize how model inputs are\n",
      " |      generated from the data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: Dataset\n",
      " |        the data to iterate\n",
      " |      epochs: int\n",
      " |        the number of times to iterate over the full dataset\n",
      " |      mode: str\n",
      " |        allowed values are 'fit' (called during training), 'predict' (called\n",
      " |        during prediction), and 'uncertainty' (called during uncertainty\n",
      " |        prediction)\n",
      " |      deterministic: bool\n",
      " |        whether to iterate over the dataset in order, or randomly shuffle the\n",
      " |        data for each epoch\n",
      " |      pad_batches: bool\n",
      " |        whether to pad each batch up to this model's preferred batch size\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a generator that iterates batches, each represented as a tuple of lists:\n",
      " |      ([inputs], [outputs], [weights])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from deepchem.models.keras_model.KerasModel:\n",
      " |  \n",
      " |  compute_saliency(self, X: numpy.ndarray) -> Union[numpy.ndarray, Sequence[numpy.ndarray]]\n",
      " |      Compute the saliency map for an input sample.\n",
      " |      \n",
      " |      This computes the Jacobian matrix with the derivative of each output element\n",
      " |      with respect to each input element.  More precisely,\n",
      " |      \n",
      " |      - If this model has a single output, it returns a matrix of shape\n",
      " |        (output_shape, input_shape) with the derivatives.\n",
      " |      - If this model has multiple outputs, it returns a list of matrices, one\n",
      " |        for each output.\n",
      " |      \n",
      " |      This method cannot be used on models that take multiple inputs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: ndarray\n",
      " |        the input data for a single sample\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      the Jacobian matrix, or a list of matrices\n",
      " |  \n",
      " |  evaluate_generator(self, generator: Iterable[Tuple[Any, Any, Any]], metrics: List[deepchem.metrics.metric.Metric], transformers: List[transformers.Transformer] = [], per_task_metrics: bool = False)\n",
      " |      Evaluate the performance of this model on the data produced by a generator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generator: generator\n",
      " |        this should generate batches, each represented as a tuple of the form\n",
      " |        (inputs, labels, weights).\n",
      " |      metric: list of deepchem.metrics.Metric\n",
      " |        Evaluation metric\n",
      " |      transformers: list of dc.trans.Transformers\n",
      " |        Transformers that the input data has been transformed by.  The output\n",
      " |        is passed through these transformers to undo the transformations.\n",
      " |      per_task_metrics: bool\n",
      " |        If True, return per-task scores.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |        Maps tasks to scores under metric.\n",
      " |  \n",
      " |  fit(self, dataset: deepchem.data.datasets.Dataset, nb_epoch: int = 10, max_checkpoints_to_keep: int = 5, checkpoint_interval: int = 1000, deterministic: bool = False, restore: bool = False, variables: Union[List[tensorflow.python.ops.variables.Variable], NoneType] = None, loss: Union[Callable[[List, List, List], Any], NoneType] = None, callbacks: Union[Callable, List[Callable]] = [], all_losses: Union[List[float], NoneType] = None) -> float\n",
      " |      Train this model on a dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: Dataset\n",
      " |        the Dataset to train on\n",
      " |      nb_epoch: int\n",
      " |        the number of epochs to train for\n",
      " |      max_checkpoints_to_keep: int\n",
      " |        the maximum number of checkpoints to keep.  Older checkpoints are discarded.\n",
      " |      checkpoint_interval: int\n",
      " |        the frequency at which to write checkpoints, measured in training steps.\n",
      " |        Set this to 0 to disable automatic checkpointing.\n",
      " |      deterministic: bool\n",
      " |        if True, the samples are processed in order.  If False, a different random\n",
      " |        order is used for each epoch.\n",
      " |      restore: bool\n",
      " |        if True, restore the model from the most recent checkpoint and continue training\n",
      " |        from there.  If False, retrain the model from scratch.\n",
      " |      variables: list of tf.Variable\n",
      " |        the variables to train.  If None (the default), all trainable variables in\n",
      " |        the model are used.\n",
      " |      loss: function\n",
      " |        a function of the form f(outputs, labels, weights) that computes the loss\n",
      " |        for each batch.  If None (the default), the model's standard loss function\n",
      " |        is used.\n",
      " |      callbacks: function or list of functions\n",
      " |        one or more functions of the form f(model, step) that will be invoked after\n",
      " |        every step.  This can be used to perform validation, logging, etc.\n",
      " |      all_losses: Optional[List[float]], optional (default None)\n",
      " |        If specified, all logged losses are appended into this list. Note that\n",
      " |        you can call `fit()` repeatedly with the same list and losses will\n",
      " |        continue to be appended.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The average loss over the most recent checkpoint interval\n",
      " |  \n",
      " |  fit_generator(self, generator: Iterable[Tuple[Any, Any, Any]], max_checkpoints_to_keep: int = 5, checkpoint_interval: int = 1000, restore: bool = False, variables: Union[List[tensorflow.python.ops.variables.Variable], NoneType] = None, loss: Union[Callable[[List, List, List], Any], NoneType] = None, callbacks: Union[Callable, List[Callable]] = [], all_losses: Union[List[float], NoneType] = None) -> float\n",
      " |      Train this model on data from a generator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generator: generator\n",
      " |        this should generate batches, each represented as a tuple of the form\n",
      " |        (inputs, labels, weights).\n",
      " |      max_checkpoints_to_keep: int\n",
      " |        the maximum number of checkpoints to keep.  Older checkpoints are discarded.\n",
      " |      checkpoint_interval: int\n",
      " |        the frequency at which to write checkpoints, measured in training steps.\n",
      " |        Set this to 0 to disable automatic checkpointing.\n",
      " |      restore: bool\n",
      " |        if True, restore the model from the most recent checkpoint and continue training\n",
      " |        from there.  If False, retrain the model from scratch.\n",
      " |      variables: list of tf.Variable\n",
      " |        the variables to train.  If None (the default), all trainable variables in\n",
      " |        the model are used.\n",
      " |      loss: function\n",
      " |        a function of the form f(outputs, labels, weights) that computes the loss\n",
      " |        for each batch.  If None (the default), the model's standard loss function\n",
      " |        is used.\n",
      " |      callbacks: function or list of functions\n",
      " |        one or more functions of the form f(model, step) that will be invoked after\n",
      " |        every step.  This can be used to perform validation, logging, etc.\n",
      " |      all_losses: Optional[List[float]], optional (default None)\n",
      " |        If specified, all logged losses are appended into this list. Note that\n",
      " |        you can call `fit()` repeatedly with the same list and losses will\n",
      " |        continue to be appended.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The average loss over the most recent checkpoint interval\n",
      " |  \n",
      " |  fit_on_batch(self, X: Sequence, y: Sequence, w: Sequence, variables: Union[List[tensorflow.python.ops.variables.Variable], NoneType] = None, loss: Union[Callable[[List, List, List], Any], NoneType] = None, callbacks: Union[Callable, List[Callable]] = [], checkpoint: bool = True, max_checkpoints_to_keep: int = 5) -> float\n",
      " |      Perform a single step of training.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: ndarray\n",
      " |        the inputs for the batch\n",
      " |      y: ndarray\n",
      " |        the labels for the batch\n",
      " |      w: ndarray\n",
      " |        the weights for the batch\n",
      " |      variables: list of tf.Variable\n",
      " |        the variables to train.  If None (the default), all trainable variables in\n",
      " |        the model are used.\n",
      " |      loss: function\n",
      " |        a function of the form f(outputs, labels, weights) that computes the loss\n",
      " |        for each batch.  If None (the default), the model's standard loss function\n",
      " |        is used.\n",
      " |      callbacks: function or list of functions\n",
      " |        one or more functions of the form f(model, step) that will be invoked after\n",
      " |        every step.  This can be used to perform validation, logging, etc.\n",
      " |      checkpoint: bool\n",
      " |        if true, save a checkpoint after performing the training step\n",
      " |      max_checkpoints_to_keep: int\n",
      " |        the maximum number of checkpoints to keep.  Older checkpoints are discarded.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      the loss on the batch\n",
      " |  \n",
      " |  get_checkpoints(self, model_dir: Union[str, NoneType] = None)\n",
      " |      Get a list of all available checkpoint files.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model_dir: str, default None\n",
      " |        Directory to get list of checkpoints from. Reverts to self.model_dir if None\n",
      " |  \n",
      " |  get_global_step(self) -> int\n",
      " |      Get the number of steps of fitting that have been performed.\n",
      " |  \n",
      " |  load_from_pretrained(self, source_model: 'KerasModel', assignment_map: Union[Dict[Any, Any], NoneType] = None, value_map: Union[Dict[Any, Any], NoneType] = None, checkpoint: Union[str, NoneType] = None, model_dir: Union[str, NoneType] = None, include_top: bool = True, inputs: Union[Sequence[Any], NoneType] = None, **kwargs) -> None\n",
      " |      Copies variable values from a pretrained model. `source_model` can either\n",
      " |      be a pretrained model or a model with the same architecture. `value_map`\n",
      " |      is a variable-value dictionary. If no `value_map` is provided, the variable\n",
      " |      values are restored to the `source_model` from a checkpoint and a default\n",
      " |      `value_map` is created. `assignment_map` is a dictionary mapping variables\n",
      " |      from the `source_model` to the current model. If no `assignment_map` is\n",
      " |      provided, one is made from scratch and assumes the model is composed of\n",
      " |      several different layers, with the final one being a dense layer. include_top\n",
      " |      is used to control whether or not the final dense layer is used. The default\n",
      " |      assignment map is useful in cases where the type of task is different\n",
      " |      (classification vs regression) and/or number of tasks in the setting.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source_model: dc.KerasModel, required\n",
      " |        source_model can either be the pretrained model or a dc.KerasModel with\n",
      " |        the same architecture as the pretrained model. It is used to restore from\n",
      " |        a checkpoint, if value_map is None and to create a default assignment map\n",
      " |        if assignment_map is None\n",
      " |      assignment_map: Dict, default None\n",
      " |        Dictionary mapping the source_model variables and current model variables\n",
      " |      value_map: Dict, default None\n",
      " |        Dictionary containing source_model trainable variables mapped to numpy\n",
      " |        arrays. If value_map is None, the values are restored and a default\n",
      " |        variable map is created using the restored values\n",
      " |      checkpoint: str, default None\n",
      " |        the path to the checkpoint file to load.  If this is None, the most recent\n",
      " |        checkpoint will be chosen automatically.  Call get_checkpoints() to get a\n",
      " |        list of all available checkpoints\n",
      " |      model_dir: str, default None\n",
      " |        Restore model from custom model directory if needed\n",
      " |      include_top: bool, default True\n",
      " |          if True, copies the weights and bias associated with the final dense\n",
      " |          layer. Used only when assignment map is None\n",
      " |      inputs: List, input tensors for model\n",
      " |          if not None, then the weights are built for both the source and self.\n",
      " |          This option is useful only for models that are built by\n",
      " |          subclassing tf.keras.Model, and not using the functional API by tf.keras\n",
      " |  \n",
      " |  predict(self, dataset: deepchem.data.datasets.Dataset, transformers: List[transformers.Transformer] = [], outputs: Union[tensorflow.python.framework.ops.Tensor, Sequence[tensorflow.python.framework.ops.Tensor], NoneType] = None, output_types: Union[List[str], NoneType] = None) -> Union[numpy.ndarray, Sequence[numpy.ndarray]]\n",
      " |      Uses self to make predictions on provided Dataset object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: dc.data.Dataset\n",
      " |        Dataset to make prediction on\n",
      " |      transformers: list of dc.trans.Transformers\n",
      " |        Transformers that the input data has been transformed by.  The output\n",
      " |        is passed through these transformers to undo the transformations.\n",
      " |      outputs: Tensor or list of Tensors\n",
      " |        The outputs to return.  If this is None, the model's standard prediction\n",
      " |        outputs will be returned.  Alternatively one or more Tensors within the\n",
      " |        model may be specified, in which case the output of those Tensors will be\n",
      " |        returned.\n",
      " |      output_types: String or list of Strings\n",
      " |        If specified, all outputs of this type will be retrieved\n",
      " |        from the model. If output_types is specified, outputs must\n",
      " |        be None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a NumPy array of the model produces a single output, or a list of arrays\n",
      " |      if it produces multiple outputs\n",
      " |  \n",
      " |  predict_embedding(self, dataset: deepchem.data.datasets.Dataset) -> Union[numpy.ndarray, Sequence[numpy.ndarray]]\n",
      " |      Predicts embeddings created by underlying model if any exist.\n",
      " |      An embedding must be specified to have `output_type` of\n",
      " |      `'embedding'` in the model definition.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: dc.data.Dataset\n",
      " |        Dataset to make prediction on\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a NumPy array of the embeddings model produces, or a list\n",
      " |      of arrays if it produces multiple embeddings\n",
      " |  \n",
      " |  predict_on_batch(self, X: Union[numpy.ndarray, Sequence], transformers: List[transformers.Transformer] = [], outputs: Union[tensorflow.python.framework.ops.Tensor, Sequence[tensorflow.python.framework.ops.Tensor], NoneType] = None) -> Union[numpy.ndarray, Sequence[numpy.ndarray]]\n",
      " |      Generates predictions for input samples, processing samples in a batch.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: ndarray\n",
      " |        the input data, as a Numpy array.\n",
      " |      transformers: list of dc.trans.Transformers\n",
      " |        Transformers that the input data has been transformed by.  The output\n",
      " |        is passed through these transformers to undo the transformations.\n",
      " |      outputs: Tensor or list of Tensors\n",
      " |        The outputs to return.  If this is None, the model's standard prediction\n",
      " |        outputs will be returned.  Alternatively one or more Tensors within the\n",
      " |        model may be specified, in which case the output of those Tensors will be\n",
      " |        returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a NumPy array of the model produces a single output, or a list of arrays\n",
      " |      if it produces multiple outputs\n",
      " |  \n",
      " |  predict_on_generator(self, generator: Iterable[Tuple[Any, Any, Any]], transformers: List[transformers.Transformer] = [], outputs: Union[tensorflow.python.framework.ops.Tensor, Sequence[tensorflow.python.framework.ops.Tensor], NoneType] = None, output_types: Union[str, Sequence[str], NoneType] = None) -> Union[numpy.ndarray, Sequence[numpy.ndarray]]\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generator: generator\n",
      " |        this should generate batches, each represented as a tuple of the form\n",
      " |        (inputs, labels, weights).\n",
      " |      transformers: list of dc.trans.Transformers\n",
      " |        Transformers that the input data has been transformed by.  The output\n",
      " |        is passed through these transformers to undo the transformations.\n",
      " |      outputs: Tensor or list of Tensors\n",
      " |        The outputs to return.  If this is None, the model's\n",
      " |        standard prediction outputs will be returned.\n",
      " |        Alternatively one or more Tensors within the model may be\n",
      " |        specified, in which case the output of those Tensors will\n",
      " |        be returned. If outputs is specified, output_types must be\n",
      " |        None.\n",
      " |      output_types: String or list of Strings\n",
      " |        If specified, all outputs of this type will be retrieved\n",
      " |        from the model. If output_types is specified, outputs must\n",
      " |        be None.\n",
      " |      Returns:\n",
      " |        a NumPy array of the model produces a single output, or a list of arrays\n",
      " |        if it produces multiple outputs\n",
      " |  \n",
      " |  predict_uncertainty(self, dataset: deepchem.data.datasets.Dataset, masks: int = 50) -> Union[Tuple[numpy.ndarray, numpy.ndarray], Sequence[Tuple[numpy.ndarray, numpy.ndarray]]]\n",
      " |      Predict the model's outputs, along with the uncertainty in each one.\n",
      " |      \n",
      " |      The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.\n",
      " |      It involves repeating the prediction many times with different dropout masks.\n",
      " |      The prediction is computed as the average over all the predictions.  The\n",
      " |      uncertainty includes both the variation among the predicted values (epistemic\n",
      " |      uncertainty) and the model's own estimates for how well it fits the data\n",
      " |      (aleatoric uncertainty).  Not all models support uncertainty prediction.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: dc.data.Dataset\n",
      " |        Dataset to make prediction on\n",
      " |      masks: int\n",
      " |        the number of dropout masks to average over\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      for each output, a tuple (y_pred, y_std) where y_pred is the predicted\n",
      " |      value of the output, and each element of y_std estimates the standard\n",
      " |      deviation of the corresponding element of y_pred\n",
      " |  \n",
      " |  predict_uncertainty_on_batch(self, X: Sequence, masks: int = 50) -> Union[Tuple[numpy.ndarray, numpy.ndarray], Sequence[Tuple[numpy.ndarray, numpy.ndarray]]]\n",
      " |      Predict the model's outputs, along with the uncertainty in each one.\n",
      " |      \n",
      " |      The uncertainty is computed as described in https://arxiv.org/abs/1703.04977.\n",
      " |      It involves repeating the prediction many times with different dropout masks.\n",
      " |      The prediction is computed as the average over all the predictions.  The\n",
      " |      uncertainty includes both the variation among the predicted values (epistemic\n",
      " |      uncertainty) and the model's own estimates for how well it fits the data\n",
      " |      (aleatoric uncertainty).  Not all models support uncertainty prediction.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X: ndarray\n",
      " |        the input data, as a Numpy array.\n",
      " |      masks: int\n",
      " |        the number of dropout masks to average over\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      for each output, a tuple (y_pred, y_std) where y_pred is the predicted\n",
      " |      value of the output, and each element of y_std estimates the standard\n",
      " |      deviation of the corresponding element of y_pred\n",
      " |  \n",
      " |  restore(self, checkpoint: Union[str, NoneType] = None, model_dir: Union[str, NoneType] = None) -> None\n",
      " |      Reload the values of all variables from a checkpoint file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      checkpoint: str\n",
      " |        the path to the checkpoint file to load.  If this is None, the most recent\n",
      " |        checkpoint will be chosen automatically.  Call get_checkpoints() to get a\n",
      " |        list of all available checkpoints.\n",
      " |      model_dir: str, default None\n",
      " |        Directory to restore checkpoint from. If None, use self.model_dir.\n",
      " |  \n",
      " |  save_checkpoint(self, max_checkpoints_to_keep: int = 5, model_dir: Union[str, NoneType] = None) -> None\n",
      " |      Save a checkpoint to disk.\n",
      " |      \n",
      " |      Usually you do not need to call this method, since fit() saves checkpoints\n",
      " |      automatically.  If you have disabled automatic checkpointing during fitting,\n",
      " |      this can be called to manually write checkpoints.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      max_checkpoints_to_keep: int\n",
      " |        the maximum number of checkpoints to keep.  Older checkpoints are discarded.\n",
      " |      model_dir: str, default None\n",
      " |        Model directory to save checkpoint to. If None, revert to self.model_dir\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from deepchem.models.models.Model:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  evaluate(self, dataset: deepchem.data.datasets.Dataset, metrics: List[deepchem.metrics.metric.Metric], transformers: List[transformers.Transformer] = [], per_task_metrics: bool = False, use_sample_weights: bool = False, n_classes: int = 2)\n",
      " |      Evaluates the performance of this model on specified dataset.\n",
      " |      \n",
      " |      This function uses `Evaluator` under the hood to perform model\n",
      " |      evaluation. As a result, it inherits the same limitations of\n",
      " |      `Evaluator`. Namely, that only regression and classification\n",
      " |      models can be evaluated in this fashion. For generator models, you\n",
      " |      will need to overwrite this method to perform a custom evaluation.\n",
      " |      \n",
      " |      Keyword arguments specified here will be passed to\n",
      " |      `Evaluator.compute_model_performance`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset: Dataset\n",
      " |        Dataset object.\n",
      " |      metrics: Metric / List[Metric] / function\n",
      " |        The set of metrics provided. This class attempts to do some\n",
      " |        intelligent handling of input. If a single `dc.metrics.Metric`\n",
      " |        object is provided or a list is provided, it will evaluate\n",
      " |        `self.model` on these metrics. If a function is provided, it is\n",
      " |        assumed to be a metric function that this method will attempt to\n",
      " |        wrap in a `dc.metrics.Metric` object. A metric function must\n",
      " |        accept two arguments, `y_true, y_pred` both of which are\n",
      " |        `np.ndarray` objects and return a floating point score. The\n",
      " |        metric function may also accept a keyword argument\n",
      " |        `sample_weight` to account for per-sample weights.\n",
      " |      transformers: List[Transformer]\n",
      " |        List of `dc.trans.Transformer` objects. These transformations\n",
      " |        must have been applied to `dataset` previously. The dataset will\n",
      " |        be untransformed for metric evaluation.\n",
      " |      per_task_metrics: bool, optional (default False)\n",
      " |        If true, return computed metric for each task on multitask dataset.\n",
      " |      use_sample_weights: bool, optional (default False)\n",
      " |        If set, use per-sample weights `w`.\n",
      " |      n_classes: int, optional (default None)\n",
      " |        If specified, will use `n_classes` as the number of unique classes\n",
      " |        in `self.dataset`. Note that this argument will be ignored for\n",
      " |        regression metrics.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      multitask_scores: dict\n",
      " |        Dictionary mapping names of metrics to metric scores.\n",
      " |      all_task_scores: dict, optional\n",
      " |        If `per_task_metrics == True` is passed as a keyword argument,\n",
      " |        then returns a second dictionary of scores for each task\n",
      " |        separately.\n",
      " |  \n",
      " |  get_num_tasks(self) -> int\n",
      " |      Get number of tasks.\n",
      " |  \n",
      " |  get_task_type(self) -> str\n",
      " |      Currently models can only be classifiers or regressors.\n",
      " |  \n",
      " |  reload(self) -> None\n",
      " |      Reload trained model from disk.\n",
      " |  \n",
      " |  save(self) -> None\n",
      " |      Dispatcher function for saving.\n",
      " |      \n",
      " |      Each subclass is responsible for overriding this method.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from deepchem.models.models.Model:\n",
      " |  \n",
      " |  get_model_filename(model_dir: str) -> str\n",
      " |      Given model directory, obtain filename for the model itself.\n",
      " |  \n",
      " |  get_params_filename(model_dir: str) -> str\n",
      " |      Given model directory, obtain filename for the model itself.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dc.models.MultitaskClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dc.models.MultitaskClassifier(n_tasks = 12, n_features = 1024, layer_sizes = [1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48193400700887046"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, nb_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = model.evaluate(train_dataset, [metric], transformers)\n",
    "test_scores = model.evaluate(test_dataset, [metric], transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean-roc_auc_score': 0.9578008260735348} {'mean-roc_auc_score': 0.6835958012859433}\n"
     ]
    }
   ],
   "source": [
    "print (train_scores, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
